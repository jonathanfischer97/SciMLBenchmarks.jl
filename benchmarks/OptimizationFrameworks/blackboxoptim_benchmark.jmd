---
title: Black-Box Optimization Benchmarking (BBOB) with Optimization.jl
author: Jonathan Fischer
---

This benchmark evaluates various black-box optimization algorithms from different Julia packages on standardized test functions from the BBOB (Black-Box Optimization Benchmarking) suite. We use the unified Optimization.jl interface to access optimizers from multiple packages, enabling fair comparisons across different implementations.

The BBOB test functions include a diverse set of challenging optimization problems with characteristics such as multi-modality, ill-conditioning, and non-separability - making them ideal for evaluating black-box optimization algorithms.

## Setup

We'll use the following packages:

```julia
# Load basic utility packages first
using LinearAlgebra, Random, BenchmarkTools

# Load BlackBoxOptimizationBenchmarking and Optimization packages
using BlackBoxOptimizationBenchmarking
using Optimization

# Load optimizer packages
using OptimizationBBO
using OptimizationOptimJL 
using OptimizationNLopt

# Define global RNG for reproducibility (instead of setting global seed)
const rng = Random.MersenneTwister(42)

# Create BBOB alias for convenience
const BBOB = BlackBoxOptimizationBenchmarking

# Note: For black-box optimization benchmarks, we don't need automatic differentiation,
# so we'll explicitly set autodiff=:none to avoid potential backend dependencies.
# This is especially helpful in environments where packages like ReverseDiff may not be available.
```

## BBOB Function Overview

The BBOB suite consists of 24 functions with varying characteristics. Let's first examine what functions are available and select a subset for our benchmarks:

```julia
# Get all BBOB functions
bbob_functions = BBOB.list_functions()

# Print the names of the functions
println("Available BBOB Functions:")
for (i, f) in enumerate(bbob_functions)
    println("$i. $(f.name)")
end

# Select a subset for our benchmarks
test_functions = bbob_functions[1:10]  # First 10 functions
```

## Optimizer Setup

We'll test optimizers from multiple packages, all accessed through the Optimization.jl interface:

```julia
# BBO optimizers
bbo_optimizers = [
    BBO_adaptive_de_rand_1_bin_radiuslimited(),
    BBO_de_rand_1_bin(),
    BBO_de_rand_2_bin(),
    BBO_separable_nes(),
    BBO_xnes()
]

bbo_names = [
    "BBO: DE Adaptive",
    "BBO: DE Rand 1 Bin",
    "BBO: DE Rand 2 Bin", 
    "BBO: Separable NES",
    "BBO: xNES"
]

# OptimJL optimizers
optimjl_optimizers = [
    NelderMead(),
    SAMIN(rt=0.5, verbosity=0)
]

optimjl_names = [
    "OptimJL: Nelder-Mead",
    "OptimJL: SAMIN"
]

# NLopt optimizers (using Algorithm enums)
nlopt_optimizers = [
    NLopt.GN_CRS2_LM, # Pass the Algorithm enum directly
    NLopt.GN_ESCH     # Pass the Algorithm enum directly
]

nlopt_names = [
    "NLopt: CRS2",
    "NLopt: ESCH"
]

# Combine all optimizers and names
all_optimizers = vcat(bbo_optimizers, optimjl_optimizers, nlopt_optimizers)
all_names = vcat(bbo_names, optimjl_names, nlopt_names)
```

## Benchmarking Functions

Now, let's define the functions to run our benchmarks:

```julia
"""
Run a single optimization trial and return performance metrics.
Accepts an `autodiff` setting for problem creation.
"""
function run_optimization_trial(f::BBOB.BBOBFunction, optimizer, dimension::Int, max_iters::Int, rng::Random.AbstractRNG; autodiff=:none, maxtime=30.0)
    # Parameter validation
    dimension <= 0 && throw(ArgumentError("dimension must be positive"))
    max_iters <= 0 && throw(ArgumentError("max_iters must be positive"))
    maxtime <= 0 && throw(ArgumentError("maxtime must be positive"))
    
    Δf = 1e-6  # Success threshold from BBOB specifications
    
    x0 = 10 * rand(rng, dimension) .- 5
    prob = BBOB.to_optimization_problem(f, dimension; autodiff=autodiff, x0=x0)
    
    timed_result = @btimed solve($prob, $optimizer; maxiters=$max_iters, maxtime=$maxtime)
    
    result = timed_result.value
    time_trial = timed_result.time
    
    if isnothing(result) || !hasproperty(result, :u) || !hasproperty(result, :objective)
        @warn "Invalid optimization result" optimizer=typeof(optimizer) function_name=f.name
        return (
            objective = Inf,
            solution = fill(NaN, dimension),
            success = false,
            distance = Inf,
            runtime = time_trial,
            error = true,
            nfe = NaN
        )
    end

    dist_to_min = if length(f.x_opt) >= dimension
        norm(result.u - f.x_opt[1:dimension])
    else
        @warn "Insufficient optimal points" function_name=f.name dimension=dimension available=length(f.x_opt)
        Inf
    end

    is_success = result.objective < Δf + f.f_opt
    nfe = hasproperty(result, :stats) && hasproperty(result.stats, :nf) ? result.stats.nf : NaN
    
    return (
        objective = result.objective,
        solution = result.u,
        success = is_success,
        distance = dist_to_min,
        runtime = time_trial,
        error = false,
        nfe = nfe
    )
end

"""
Run multiple optimization trials and aggregate results.
Accepts an `autodiff` setting to pass down.
"""
function run_benchmark(f::BBOB.BBOBFunction, optimizer, dimension::Int, max_iters::Int, n_trials::Int; autodiff=:none, maxtime=30.0)
    results = [run_optimization_trial(f, optimizer, dimension, max_iters, rng; autodiff=autodiff, maxtime=maxtime) for _ in 1:n_trials]
    
    valid_results = filter(r -> !r.error, results)
    n_valid = length(valid_results)

    if n_valid == 0
        println("    Warning: All trials failed or yielded invalid results.")
        return (
            success_rate = 0.0,
            mean_objective = Inf,
            mean_distance = Inf,
            mean_runtime = NaN,
            best_objective = Inf,
            raw_results = results
        )
    end

    success_values = [r.success for r in valid_results]
    objective_values = [r.objective for r in valid_results]
    distance_values = [r.distance for r in valid_results]
    runtime_values = [r.runtime for r in valid_results]

    success_rate = sum(success_values) / n_trials 
    mean_obj = sum(objective_values) / n_valid
    finite_distances = filter(isfinite, distance_values)
    mean_dist = isempty(finite_distances) ? Inf : sum(finite_distances) / length(finite_distances)
    finite_runtimes = filter(!isnan, runtime_values)
    mean_runtime = isempty(finite_runtimes) ? NaN : sum(finite_runtimes) / length(finite_runtimes)
    best_obj = minimum(objective_values)
    
    nfe_values = [r.nfe for r in valid_results if !isnan(r.nfe)]
    mean_nfe = isempty(nfe_values) ? NaN : sum(nfe_values) / length(nfe_values)
    
    return (
        success_rate = success_rate,
        mean_objective = mean_obj,
        mean_distance = mean_dist,
        mean_runtime = mean_runtime,
        best_objective = best_obj,
        raw_results = results,
        mean_nfe = mean_nfe
    )
end

"""
Run benchmarks for multiple optimizers on multiple functions.
Determines appropriate autodiff setting based on optimizer.
"""
function run_comparison(functions, optimizers, names, dimension, max_iters, n_trials, maxtime=30.0)
    results = Dict{String, Dict{String, Any}}()
    
    total_combinations = length(functions) * length(optimizers)
    println("Starting benchmark comparisons for $total_combinations function/optimizer pairs...")
    
    for f_idx in eachindex(functions)
        f = functions[f_idx]
        results[f.name] = Dict{String, Any}()
        
        for (i, optimizer) in enumerate(optimizers)
            name = names[i]
            
            current_autodiff = any(T -> typeof(optimizer) <: T, [NelderMead]) ? 
                Optimization.AutoForwardDiff() : :none
                
            @info "Testing optimizer" function_name=f.name optimizer_name=name autodiff_setting=current_autodiff
            
            time_start = time()
            try
                results[f.name][name] = run_benchmark(f, optimizer, dimension, max_iters, n_trials; 
                    autodiff=current_autodiff, maxtime=maxtime)
            catch e
                @error "Benchmark failed for $(f.name) with $name" error_details=e
                results[f.name][name] = (
                    success_rate = 0.0,
                    mean_objective = Inf,
                    mean_distance = Inf,
                    mean_runtime = NaN,
                    best_objective = Inf,
                    raw_results = []
                )
            end
            time_end = time()
            println("    Finished $(f.name) with $name in $(round(time_end - time_start, digits=2)) seconds.")
        end
    end
    
    return results
end
```

## Running the Benchmarks

Now we'll run the benchmarks with the following parameters:

```julia
# Benchmark parameters
dimension = 5        # Standard BBOB test dimension
max_iters = 5000    # Increased budget for better convergence
n_trials = 1       # For statistical significance
maxtime = 30.0      # Maximum time per trial in seconds

# Run the benchmarks
benchmark_results = run_comparison(
    test_functions,
    all_optimizers,
    all_names,
    dimension,
    max_iters,
    n_trials,
    maxtime
)
```

## Results Analysis

Let's analyze and visualize the results:

```julia
# Load plotting package in this block
using Plots
# Make sure we have the right backend configuration
gr(size=(800, 600), dpi=300, fmt=:png)

# Visualize success heatmap
function plot_success_heatmap(results, functions, names)
    # Create a heatmap of success rates
    success_rates = [results[f.name][name].success_rate for f in functions for name in names]
    heatmap(success_rates, aspect_ratio=:equal, c=:viridis, xlabel="Function", ylabel="Optimizer", title="Success Rate Heatmap")
end

# Visualize performance metrics
function plot_performance_metrics(results, functions, names)
    # Create separate plots for each performance metric
    metrics = [:mean_objective, :mean_distance, :mean_runtime]
    metric_labels = ["Mean Objective", "Mean Distance to Optimum", "Mean Runtime (s)"]

    for (idx, metric) in enumerate(metrics)
        metric_data = [results[f.name][name][metric] for f in functions, name in names]
        
        p = heatmap(metric_data, 
                    xticks=(1:length(names), names),
                    yticks=(1:length(functions), [f.name for f in functions]),
                    xlabel="Optimizer", 
                    ylabel="BBOB Function", 
                    title="Performance: $(metric_labels[idx])",
                    xrotation=45,
                    yrotation=45, # Adjust if names are long
                    margin=10Plots.mm, # Add margin for labels
                    c=:viridis, aspect_ratio=:auto) # Use heatmap for clarity
        
        display(p)
        println("Performance metric plot for $(metric_labels[idx]) generated.")
    end
end

# Call the functions to create plots
# plot_success_heatmap(benchmark_results, test_functions, all_names)
display(plot_success_heatmap(benchmark_results, test_functions, all_names))
println("Success heatmap generated.")
plot_performance_metrics(benchmark_results, test_functions, all_names)
```

## Detailed Analysis

Let's look at detailed performance statistics for each optimizer:

```julia
function print_summary_table(results, functions, names)
    # Calculate aggregate statistics without using Statistics
    optimizer_stats = Dict{String, Dict{Symbol, Float64}}()
    
    for name in names
        success_rates = Float64[]
        distances = Float64[]
        runtimes = Float64[]
        
        for f in functions
            if haskey(results[f.name], name)
                r = results[f.name][name]
                push!(success_rates, r.success_rate)
                push!(distances, r.mean_distance)
                push!(runtimes, r.mean_runtime)
            end
        end
        
        # Manual calculation of means
        n_success = length(success_rates)
        n_distance = length(distances)
        n_runtime = length(runtimes)
        
        avg_success = n_success > 0 ? sum(success_rates) / n_success : 0.0
        avg_distance = n_distance > 0 ? sum(distances) / n_distance : 0.0
        avg_runtime = n_runtime > 0 ? sum(runtimes) / n_runtime : 0.0
        
        optimizer_stats[name] = Dict(
            :avg_success => avg_success,
            :avg_distance => avg_distance,
            :avg_runtime => avg_runtime
        )
    end
    
    # Sort by success rate
    sorted_optimizers = sort(collect(names), by=name -> -optimizer_stats[name][:avg_success])
    
    # Print the table
    println("| Optimizer | Success Rate | Mean Distance | Mean Runtime |")
    println("|-----------|--------------|---------------|-------------|")
    
    for name in sorted_optimizers
        stats = optimizer_stats[name]
        println("| $(rpad(name, 20)) | $(round(stats[:avg_success], digits=2)) | $(round(stats[:avg_distance], digits=4)) | $(round(stats[:avg_runtime], digits=4)) |")
    end
end

print_summary_table(benchmark_results, test_functions, all_names)
```

## Case Study: Rastrigin Function

Let's take a closer look at the Rastrigin function, which is known for its many local minima:

```julia
# Find the Rastrigin function correctly by name
rastrigin_idx = findfirst(f -> occursin("Rastrigin", f.name), test_functions)
if isnothing(rastrigin_idx)
    # Fallback if not found by name - use a reasonable default
    println("Warning: Rastrigin function not found by name, using test_functions[3] as fallback")
    rastrigin = test_functions[3]
else
    rastrigin = test_functions[rastrigin_idx]
    println("Found Rastrigin function: $(rastrigin.name)")
end

# Run a more detailed benchmark on Rastrigin
rastrigin_results = Dict{String, Any}()
optimizers_requiring_ad = [NelderMead] # Redefine or pass from outside

for (i, optimizer) in enumerate(all_optimizers)
    name = all_names[i]
    
    current_autodiff = :none
    if any(T -> typeof(optimizer) <: T, optimizers_requiring_ad)
         println("Testing $name on Rastrigin (using autodiff=AutoForwardDiff)...")
         current_autodiff = Optimization.AutoForwardDiff()
    else
         println("Testing $name on Rastrigin (using autodiff=:none)...")
         current_autodiff = :none
    end

    try
        # Use dimension=2 for Rastrigin case study and pass autodiff
        rastrigin_results[name] = run_benchmark(rastrigin, optimizer, 2, 20000, 20; autodiff=current_autodiff, maxtime=45.0)
        
        # Print detailed results only if successful
        if rastrigin_results[name].mean_objective != Inf
            println("  Success Rate: $(rastrigin_results[name].success_rate)")
            println("  Mean Distance: $(rastrigin_results[name].mean_distance)")
            println("  Best Objective: $(rastrigin_results[name].best_objective)")
        else
             println("  Failed or yielded no valid results.")
        end
    catch e
        println("  Failed catastrophically: $e")
    end
end

# Visualize Rastrigin solutions 
function visualize_rastrigin_solutions(rastrigin, rastrigin_results, all_names)
    # Create the function plot using recipe defined in BlackBoxOptimizationBenchmarking
    p = plot(rastrigin, size=(800, 600), zoom=1.5,
             title="Rastrigin Function with Optimizer Solutions",
             xlabel="x₁", ylabel="x₂")
    
    # Plot some solution points from different optimizers
    optimizer_colors = Dict(zip(all_names, 1:length(all_names)))
    
    # Keep track of successful optimizers for the legend
    successful_optimizers = String[]
    
    for (name, results) in rastrigin_results
        if haskey(results, :raw_results) && !isempty(results.raw_results)
            # Get the best result
            objectives = [r.objective for r in results.raw_results]
            if !isempty(objectives)
                best_idx = argmin(objectives)
                best_result = results.raw_results[best_idx]
                
                if length(best_result.solution) >= 2
                    # Add the solution point to the plot
                    scatter!(
                        p,
                        [best_result.solution[1]],
                        [best_result.solution[2]],
                        label=name,
                        markersize=8,
                        markershape=:circle,
                        color=get(optimizer_colors, name, 1),
                        markerstrokewidth=2
                    )
                    push!(successful_optimizers, name)
                    
                    # Print some debugging info
                    println("Added solution for $name at coordinates ($(best_result.solution[1]), $(best_result.solution[2]))")
                else
                    println("Solution for $name doesn't have enough dimensions: $(best_result.solution)")
                end
            end
        end
    end
    
    # Add legend if we have any successful optimizers
    if !isempty(successful_optimizers)
        p = plot!(p, legend=:topleft)
    end
    
    return p
end

# Check if we have enough valid results before visualizing
println("Checking Rastrigin results availability...")
if !isempty(rastrigin_results) && any(haskey.(values(rastrigin_results), :raw_results))
    println("Found valid results. Attempting to create visualization...")
    try
        rastrigin_plot = visualize_rastrigin_solutions(rastrigin, rastrigin_results, all_names)
        println("Visualization created successfully.")
        display(rastrigin_plot)
        println("Rastrigin solution visualization generated and displayed.")
    catch e
        println("Error creating Rastrigin visualization: $e")
        println("This is likely due to the plotting backend issue and doesn't affect benchmark results.")
    end
else
    println("Not enough valid results to create Rastrigin visualization")
end
```

## Conclusion

This benchmark suite demonstrates how to use the Optimization.jl interface to evaluate multiple black-box optimization algorithms from different packages on standard test problems. The BBOB functions provide a challenging set of test cases that help distinguish the strengths and weaknesses of different optimizers.

Based on our results, we can make the following observations:

1. The differential evolution variants from BlackBoxOptim (particularly `BBO_adaptive_de_rand_1_bin_radiuslimited`) tend to perform well across many functions.
2. Some algorithms excel on specific problem types but struggle on others, highlighting the importance of algorithm selection.
3. Runtime can vary significantly between optimizers, with simpler methods often being faster but less reliable.

This benchmark provides a foundation for selecting appropriate black-box optimization algorithms for specific problem types and constraints.

## Appendix

```julia, echo = false
using SciMLBenchmarks
SciMLBenchmarks.bench_footer(WEAVE_ARGS[:folder],WEAVE_ARGS[:file])
```